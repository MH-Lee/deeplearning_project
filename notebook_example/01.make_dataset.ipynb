{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os, json, sys\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import datetime\n",
    "from ast import literal_eval\n",
    "from matplotlib import rc, rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Today = datetime.date.today().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Assign file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data save label\n",
    "TRAIN_INPUT_DATA = 'train_input.npy'\n",
    "TEST_INPUT_DATA = 'test_input.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "SEQ_CONFIGS = 'seq_configs.json'\n",
    "TOK_CONFIG = 'tokenizer_config.json'\n",
    "\n",
    "# Train label save file name\n",
    "TRAIN_LABEL = 'train_label.npy'\n",
    "TRAIN_LABEL_SMALL = 'train_label_small.npy'\n",
    "\n",
    "# Test label save file name\n",
    "TEST_LABEL = 'test_label.npy'\n",
    "TEST_LABEL_SMALL = 'test_label_small.npy'\n",
    "\n",
    "#File name\n",
    "LABEL_JSON_NAME = '../assets/label_data/label.json'\n",
    "LABEL_JSON_NAME_SMALL = '../assets/label_data/label_small.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.read_excel('../assets/data/doc_set_final_version3.xlsx')\n",
    "stop_words = pd.read_csv('../assets/data/korean_stopwords.txt')['stopwords'].tolist()\n",
    "total_data['token'] =  total_data['token'].apply(lambda x: literal_eval(x))\n",
    "total_data = total_data.sample(frac=1, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = total_data[['token', 'new_small_class']]\n",
    "token_list = total_data['token'].tolist()\n",
    "target = total_data['new_class'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Label Encoder & Train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lmhoon012\\Anaconda3\\envs\\deeplearning-gpu\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "### target class (big and small) encoding\n",
    "lbl_e = LabelEncoder()\n",
    "target_label = lbl_e.fit_transform(total_data['new_class'])\n",
    "le_name_mapping = dict(zip(lbl_e.transform(lbl_e.classes_), lbl_e.classes_))\n",
    "le_dict = dict()\n",
    "for k, v in le_name_mapping.items():\n",
    "    le_dict[str(k)] = v\n",
    "json.dump(le_dict, open(LABEL_JSON_NAME, 'w'), ensure_ascii=True)\n",
    "\n",
    "lbl_e2 = LabelEncoder()\n",
    "target_label_small = lbl_e2.fit_transform(total_data['new_small_class'])\n",
    "le_small_name_mapping = dict(zip(lbl_e2.transform(lbl_e2.classes_), lbl_e2.classes_))\n",
    "data['new_small_class_le'] = target_label_small\n",
    "le_dict2 = dict()\n",
    "for k, v in le_small_name_mapping.items():\n",
    "    le_dict2[str(k)] = v\n",
    "json.dump(le_dict2, open(LABEL_JSON_NAME_SMALL, 'w'), ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_data, test_x_data, train_y, test_y = train_test_split(data, target_label,\n",
    "                                                             test_size=0.3,\n",
    "                                                             stratify=target,\n",
    "                                                             shuffle=True,\n",
    "                                                             random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10789, 3) (4624, 3) 10789 4624\n",
      "10789 4624 43 43\n",
      "big 클래스 개수 :  43\n",
      "학습데이터 클래스 개수 :  43\n",
      "검증데이터 클래스 개수 :  43\n",
      "small 클래스 개수 :  455\n",
      "학습데이터 클래스 개수 :  455\n",
      "검증데이터 클래스 개수 :  455\n",
      "전처리 후 명사 길이 최대 값: 500\n",
      "전처리 후 명사 길이 최소 값: 70\n",
      "전처리 후 명사 길이 평균 값: 158.98\n",
      "전처리 후 명사 길이 표준편차: 59.96\n",
      "전처리 후 명사 길이 중간 값: 150.0\n",
      "전처리 후 명사 길이 제 1 사분위: 113.0\n",
      "전처리 후 명사 길이 제 3 사분위: 195.0\n"
     ]
    }
   ],
   "source": [
    "print(train_x_data.shape, test_x_data.shape, len(train_y), len(test_y))\n",
    "print(len(train_x_data.iloc[:, 0]), len(test_x_data.iloc[:, 0]), len(np.unique(train_y)), len(np.unique(test_y)))\n",
    "\n",
    "label_number = len(le_dict.keys())\n",
    "print(\"big 클래스 개수 : \", label_number)\n",
    "print(\"학습데이터 클래스 개수 : \", len(np.unique(train_y)))\n",
    "print(\"검증데이터 클래스 개수 : \",len(np.unique(test_y)))\n",
    "\n",
    "label_number_small = len(le_dict2.keys())\n",
    "print(\"small 클래스 개수 : \", label_number_small)\n",
    "print(\"학습데이터 클래스 개수 : \", len(np.unique(train_x_data['new_small_class_le'])))\n",
    "print(\"검증데이터 클래스 개수 : \", len(np.unique(test_x_data['new_small_class_le'])))\n",
    "\n",
    "after_len = [len(word) for word in train_x_data['token']]\n",
    "print('전처리 후 명사 길이 최대 값: {}'.format(np.max(after_len)))\n",
    "print('전처리 후 명사 길이 최소 값: {}'.format(np.min(after_len)))\n",
    "print('전처리 후 명사 길이 평균 값: {:.2f}'.format(np.mean(after_len)))\n",
    "print('전처리 후 명사 길이 표준편차: {:.2f}'.format(np.std(after_len)))\n",
    "print('전처리 후 명사 길이 중간 값: {}'.format(np.median(after_len)))\n",
    "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "print('전처리 후 명사 길이 제 1 사분위: {}'.format(np.percentile(after_len, 25)))\n",
    "print('전처리 후 명사 길이 제 3 사분위: {}'.format(np.percentile(after_len, 75)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. word token list conver to sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(train_x_data['token'])\n",
    "train_sequence = tokenizer.texts_to_sequences(train_x_data['token'])\n",
    "test_sequence = tokenizer.texts_to_sequences(test_x_data['token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtered lower frequency word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_count_words = [w for w,c in tokenizer.word_counts.items() if c < 5]\n",
    "for w in low_count_words:\n",
    "    del tokenizer.word_index[w]\n",
    "    del tokenizer.word_docs[w]\n",
    "    del tokenizer.word_counts[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../assets/data/npy_data/2020-05-28/ -- Folder already exists \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequence_data = dict()\n",
    "sequence_data['train_seq'] = train_sequence\n",
    "sequence_data['test_seq'] = test_sequence\n",
    "sequence_data['train_token_list'] = train_x_data['token'].tolist()\n",
    "sequence_data['test_token_list'] = test_x_data['token'].tolist()\n",
    "sequence_data['tokenizer_config'] = tokenizer.get_config()\n",
    "\n",
    "word_idx = tokenizer.word_index\n",
    "MAX_SEQUENCE_LENGTH = int(np.median(after_len))\n",
    "DATA_OUT_PATH = '../assets/data/npy_data/{}/'.format(Today)\n",
    "## Make output save directory\n",
    "if os.path.exists(DATA_OUT_PATH):\n",
    "    print(\"{} -- Folder already exists \\n\".format(DATA_OUT_PATH))\n",
    "else:\n",
    "    os.makedirs(DATA_OUT_PATH, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(DATA_OUT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sequece data padding\n",
    "\n",
    "+ Since the length of the sequence data is different, the maximum length is truncated to the median and zero padding is added to make the data equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = pad_sequences(train_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "train_labels = np.array(train_y)\n",
    "train_small_labels = np.array(train_x_data['new_small_class_le'])\n",
    "test_input = pad_sequences(test_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "test_labels = np.array(test_y)\n",
    "test_small_labels = np.array(test_x_data['new_small_class_le'])\n",
    "\n",
    "data_configs = {}\n",
    "data_configs['vocab'] = word_idx\n",
    "data_configs['vocab_size'] = len(word_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA SAVE\n",
    "# 전처리 된 데이터를 넘파이 형태로 저장\n",
    "np.save(open(DATA_OUT_PATH + TRAIN_INPUT_DATA, 'wb'), train_input)\n",
    "np.save(open(DATA_OUT_PATH + TEST_INPUT_DATA, 'wb'), test_input)\n",
    "\n",
    "# save label numpy file\n",
    "np.save(open(DATA_OUT_PATH + TRAIN_LABEL, 'wb'), train_labels)\n",
    "np.save(open(DATA_OUT_PATH + TEST_LABEL, 'wb'), test_labels)\n",
    "\n",
    "# save small label numpy file\n",
    "np.save(open(DATA_OUT_PATH + TRAIN_LABEL_SMALL, 'wb'), train_small_labels)\n",
    "np.save(open(DATA_OUT_PATH + TEST_LABEL_SMALL, 'wb'), test_small_labels)\n",
    "\n",
    "json.dump(data_configs, open(DATA_OUT_PATH + DATA_CONFIGS, 'w'), ensure_ascii=True)\n",
    "json.dump(sequence_data, open(DATA_OUT_PATH + SEQ_CONFIGS, 'w'), ensure_ascii=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
